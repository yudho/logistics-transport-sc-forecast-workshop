{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0:  Set up <a class=\"anchor\" id=\"setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard open libraries\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "# AWS libraries and initialization\n",
    "import boto3\n",
    "\n",
    "# Load variables\n",
    "%store -r df_raw\n",
    "%store -r s3_bucket_name\n",
    "%store -r s3_prefix\n",
    "%store -r start_time\n",
    "%store -r end_time\n",
    "%store -r item_id\n",
    "%store -r target_value\n",
    "%store -r timestamp\n",
    "%store -r forecast_dims\n",
    "%store -r FORECAST_FREQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Prepare Related Time Series <a class=\"anchor\" id=\"RTS\"></a>\n",
    "\n",
    "Make sure RTS does not have any missing values, even if RTS extends into future. <br>\n",
    "Trick:  create dataframe without any missing values using cross-join, faster than resample technique. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install dask for faster joins when df is large\n",
    "!pip install \"dask[dataframe]\" \n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "print('dask: {}'.format(dask.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you get memory allocation error in merges below, try overriding default value 0 to 1 for overcommit\n",
    "# see https://www.kernel.org/doc/Documentation/vm/overcommit-accounting\n",
    "# Next 2 commands - open new terminal and do these directly in terminal\n",
    "# !sudo -i \n",
    "# !echo 1 > /proc/sys/vm/overcommit_memory\n",
    "!cat /proc/sys/vm/overcommit_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.date_range(start=start_time, end=end_time, freq=FORECAST_FREQ)\n",
    "all_times = pd.DataFrame(index=idx)\n",
    "print (f\"Number of data points: {len(all_times.index)}\")\n",
    "print (f\"Start date = {all_times.index.min()}\")\n",
    "print (f\"End date = {all_times.index.max()}\")\n",
    "\n",
    "# Create timestamp column\n",
    "all_times.reset_index(inplace=True)\n",
    "all_times.columns = [timestamp]\n",
    "\n",
    "print(all_times.dtypes)\n",
    "print(all_times.isna().sum())\n",
    "print(all_times.shape)\n",
    "all_times.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# create master template of all possible locations and items\n",
    "\n",
    "items = pd.DataFrame(list(df_raw[item_id].unique()))\n",
    "items.columns = [item_id]\n",
    "# print(items.head(2))\n",
    "master_records = items.copy()\n",
    "print(master_records.shape, items.shape)\n",
    "\n",
    "# check you did the right thing\n",
    "num_items = len(master_records[item_id].value_counts())\n",
    "print(f\"num items = {num_items}\")\n",
    "master_records.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# cross-join to create master template of all possible locations and items and times\n",
    "all_times['key'] = \"1\"\n",
    "master_records['key'] = \"1\"\n",
    "all_times.set_index('key', inplace=True)\n",
    "master_records.set_index('key', inplace=True)\n",
    "\n",
    "# Do the cross-join\n",
    "print(\"doing the merge...\")\n",
    "full_history = master_records.merge(all_times, how=\"outer\", left_index=True, right_index=True)\n",
    "print(\"done w/ merge...\")\n",
    "full_history.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# make sure you don't have any nulls\n",
    "print(full_history.shape)\n",
    "print(\"checking nulls...\")\n",
    "print(full_history.isna().sum())\n",
    "full_history.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create small df of target_values - to merge later using dask\n",
    "temp_target = df_raw[forecast_dims + [target_value]].copy()\n",
    "# add key for faster join\n",
    "temp_target['ts_key'] = temp_target[timestamp].astype(str) + \"-\" + temp_target[item_id]\n",
    "temp_target = temp_target.groupby('ts_key').sum()\n",
    "# temp_target.drop(forecast_dims, inplace=True, axis=1)\n",
    "# temp_target.set_index('ts_key', inplace=True)\n",
    "print(temp_target.shape, df_raw.shape)\n",
    "display(temp_target.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Parallelization for faster merge </b><br>\n",
    "Below, I used dask.  I also tried ray through modin library.  I found error when adding a new column to modin dataframe.  Maybe by the time you use this notebook the modin/ray problem will be solved.\n",
    "https://github.com/modin-project/modin/issues/2442\n",
    "\n",
    "For reference, here are dask best practices:\n",
    "<ul>\n",
    "    <li>Choose partitions to be #items if your time series have more dimensions than just item_id, see <a href=\"https://docs.dask.org/en/latest/best-practices.html\" target=\"_blank\">https://docs.dask.org/en/latest/best-practices.html</a></li>\n",
    "    <li>Make sure reset_index is only done in pandas and not dask, see <a href=\"https://docs.dask.org/en/latest/dataframe-best-practices.html\" target=\"_blank\">https://docs.dask.org/en/latest/dataframe-best-practices.html</a></li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# USING dask\n",
    "\n",
    "# convert pandas to dask df\n",
    "print(type(full_history))\n",
    "num_partitions = 1\n",
    "print(f\"using num_partitions = {num_partitions}\")\n",
    "large_df = dd.from_pandas(full_history, npartitions=num_partitions)\n",
    "print(type(large_df))\n",
    "\n",
    "# add key for faster join with target_value\n",
    "large_df['ts_key'] = large_df[timestamp].astype(str) + \"-\" + large_df[item_id]   \n",
    "print(large_df.shape, full_history.shape)\n",
    "display(large_df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# merge in original target_value\n",
    "temp = large_df.merge(temp_target, how=\"left\", right_index=True, left_on=\"ts_key\")\n",
    "print(temp.shape, full_history.shape)\n",
    "display(temp.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# convert dask df back to pandas df\n",
    "# # Below is too small !?  rows got dropped, why?\n",
    "print(type(temp))\n",
    "temp3 = temp.compute()\n",
    "print(type(temp3))\n",
    "temp3.drop('ts_key', axis=1, inplace=True)\n",
    "print(temp3.shape, full_history.shape)\n",
    "\n",
    "# check nulls\n",
    "print(temp3.isna().sum())\n",
    "display(temp3.sample(3))\n",
    "temp3[target_value].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check you did the right thing\n",
    "# Check original target_values\n",
    "df_raw[target_value].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Careful!!\n",
    "# Really replace full_history with merged values\n",
    "print(full_history.shape)\n",
    "full_history = temp3.copy()\n",
    "print(full_history.shape)\n",
    "print(type(full_history))\n",
    "del temp, temp_target, temp3\n",
    "full_history.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Candidate variables for hourly data\n",
    "full_history['day_of_week'] = full_history[timestamp].dt.day_name().astype(str)\n",
    "full_history['hour_of_day'] = full_history[timestamp].dt.hour.astype(str)\n",
    "full_history['day_hour_name'] = full_history['day_of_week'] + \"_\" + full_history['hour_of_day']\n",
    "full_history['weekend_flag'] = full_history[timestamp].dt.dayofweek\n",
    "full_history['weekend_flag'] = (full_history['weekend_flag'] >= 5).astype(int)\n",
    "full_history['is_sun_mon'] = 0\n",
    "full_history.loc[((full_history.day_of_week==\"Sunday\") | (full_history.day_of_week==\"Monday\")), 'is_sun_mon'] = 1\n",
    "\n",
    "print(full_history.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zoom-in time slice so you can see patterns\n",
    "location_picked = \"231\"\n",
    "df_plot = full_history.loc[(full_history[item_id] == location_picked), :].copy()\n",
    "df_plot = full_history.loc[((full_history[timestamp]>\"2020-01-10\")\n",
    "                           & (full_history[timestamp]<end_time)\n",
    "                           & (full_history[item_id] == location_picked)), :].copy()\n",
    "print(df_plot.shape, full_history.shape)\n",
    "df_plot = df_plot.groupby([timestamp]).sum()\n",
    "df_plot.reset_index(inplace=True)\n",
    "df_plot.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check: target_value distribution in full dataframe looks same as original\n",
    "df_plot[target_value].hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Visualize Related Time Series <a class=\"anchor\" id=\"visualize_rts\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize candidate RTS variables\n",
    "plt.figure(figsize=(15, 8))\n",
    "ax = plt.gca()\n",
    "df_plot.plot(x=timestamp, y=target_value, ax=ax);\n",
    "ax2 = ax.twinx()\n",
    "df_plot.plot(x=timestamp, y='weekend_flag', color='red', alpha=0.3, ax=ax2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE HOURLY RTS\n",
    "\n",
    "# Visualize candidate RTS variables is_sun_mon\n",
    "plt.figure(figsize=(15, 8))\n",
    "ax = plt.gca()\n",
    "df_plot.plot(x=timestamp, y=target_value, ax=ax);\n",
    "ax2 = ax.twinx()\n",
    "df_plot.plot(x=timestamp, y='is_sun_mon', color='red', alpha=0.3, ax=ax2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like lowest taxis rides are a combination of day and hour that seems to matter, not just day of week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE HOURLY RTS\n",
    "\n",
    "# Assemble RTS - include whatever columns you finally decide\n",
    "rts = full_history[forecast_dims + ['day_hour_name']].copy()\n",
    "\n",
    "print(rts.shape)\n",
    "print(rts.isnull().sum())\n",
    "print(f\"rts start: {rts[timestamp].min()}\")\n",
    "print(f\"rts end: {rts[timestamp].max()}\")\n",
    "rts.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Save Related Time Series <a class=\"anchor\" id=\"save_rts\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save rts to S3\n",
    "local_file = \"rts.csv\"\n",
    "# Save merged file locally\n",
    "rts.to_csv(local_file, header=False, index=False)\n",
    "\n",
    "key = f\"{s3_prefix}/{local_file}\"\n",
    "boto3.Session().resource('s3').Bucket(s3_bucket_name).Object(key).upload_file(local_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store df_raw\n",
    "%store s3_bucket_name\n",
    "%store s3_prefix\n",
    "%store start_time\n",
    "%store end_time\n",
    "%store item_id\n",
    "%store target_value\n",
    "%store timestamp\n",
    "%store FORECAST_FREQ\n",
    "%store forecast_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:ap-southeast-1:492261229750:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
