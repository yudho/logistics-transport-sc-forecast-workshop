{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data used in these notebooks: NYC Taxi trips open data\n",
    "\n",
    "Given hourly historical taxi trips data for NYC, your task is to predict #pickups in next 7 days, per hour and per pickup zone.  <br>\n",
    "\n",
    "<ul>\n",
    "<li>Original data source:  <a href=\"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\" target=\"_blank\"> https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page</a> </li>\n",
    "<li>AWS-hosted public source:  <a href=\"https://registry.opendata.aws/nyc-tlc-trip-records-pds/\" target=\"_blank\">https://registry.opendata.aws/nyc-tlc-trip-records-pds/ </a> </li>\n",
    "<li>AWS managed weather data ingestion as a service that is bundled with Amazon Forecast, aggregated by location and by hour.  Initially only for USA and Europe, but depending on demand, possibly in the future for other global regions. </li>\n",
    "<li>Data used:  Yellow taxis dates: 2018-12 through 2020-02 to avoid COVID effects </li>\n",
    "</ul>\n",
    "\n",
    " \n",
    "### Features and cleaning\n",
    "Note: ~5GB Raw Data has already been cleaned and joined using AWS Glue (tutorials to be created in future). \n",
    "<ul>\n",
    "    <li>Join shape files Latitude, Longitude</li>\n",
    "    <li>Add Trip duration in minutes</li>\n",
    "    <li>Drop negative trip distances, 0 fares, 0 passengers, less than 1min trip durations </li>\n",
    "    <li>Drop 2 unknown zones ['264', '265']\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0:  Set up and install libraries <a class=\"anchor\" id=\"setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard open libraries\n",
    "import pandas as pd\n",
    "import json\n",
    "from time import sleep\n",
    "\n",
    "# AWS libraries and initialization\n",
    "import boto3\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Define S3 bucket</b></br>\n",
    "The cell below will use the default SageMaker S3 bucket in the account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "sess = sagemaker.Session()\n",
    "s3_bucket_name = sess.default_bucket()\n",
    "    \n",
    "# create prefix for organizing your new bucket\n",
    "s3_prefix = \"nyc-taxi-trips\"\n",
    "print(f\"using folder '{s3_prefix}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Read data <a class=\"anchor\" id=\"read\"></a>\n",
    "\n",
    "The first thing we're going to do is read the headerless .csv file.  Then we need to identify which columns map to required Amazon Forecast inputs.\n",
    "\n",
    "<img src=\"https://amazon-forecast-samples.s3-us-west-2.amazonaws.com/common/images/nyctaxi_map_fields.png\" width=\"82%\">\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read cleaned, joined, featurized data from Glue ETL processing\n",
    "df_raw = pd.read_csv(\"s3://amazon-forecast-samples/data_prep_templates/clean_features.csv\"\n",
    "                          , parse_dates=True\n",
    "                          , header=None\n",
    "                          , dtype={0:'str'\n",
    "                                   , 1: 'str'\n",
    "                                   , 2: 'str'\n",
    "                                   , 3:'str'\n",
    "                                   , 4: 'int32'\n",
    "                                   , 5: 'float64'\n",
    "                                   , 6: 'str'\n",
    "                                   , 7: 'str'\n",
    "                                   , 8: 'str'}\n",
    "                          , names=['pulocationid', 'pickup_hourly', 'pickup_day_of_week'\n",
    "                                   , 'day_hour', 'trip_quantity', 'mean_item_loc_weekday'\n",
    "                                   , 'pickup_geolocation', 'pickup_borough', 'binned_max_item'])\n",
    "\n",
    "# drop duplicates\n",
    "print(df_raw.shape)\n",
    "df_raw.drop_duplicates(inplace=True)\n",
    "\n",
    "df_raw['pickup_hourly'] = pd.to_datetime(df_raw[\"pickup_hourly\"], format=\"%Y-%m-%d %H:%M:%S\", errors='coerce')\n",
    "print(df_raw.shape)\n",
    "start_time = df_raw.pickup_hourly.min()\n",
    "end_time = df_raw.pickup_hourly.max()\n",
    "print(f\"Min timestamp = {start_time}\")\n",
    "print(f\"Max timestamp = {end_time}\")\n",
    "df_raw.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Transform Data <a class=\"anchor\" id=\"transform_tts\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map expected column names\n",
    "item_id = \"pulocationid\"\n",
    "target_value = \"trip_quantity\"\n",
    "timestamp = \"pickup_hourly\"\n",
    "\n",
    "# forecast setting\n",
    "FORECAST_FREQ = \"H\"\n",
    "\n",
    "# specify array of dimensions you'll use for forecasting\n",
    "forecast_dims = [timestamp, item_id]\n",
    "\n",
    "print(f\"forecast_dims = {forecast_dims}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assemble TTS required columns\n",
    "\n",
    "tts = df_raw[[timestamp, item_id, target_value]].copy()\n",
    "\n",
    "print(f\"start date = {tts[timestamp].min()}\")\n",
    "print(f\"end date = {tts[timestamp].max()}\")\n",
    "\n",
    "# check it\n",
    "print(tts.shape)\n",
    "print(tts.dtypes)\n",
    "tts.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Save Data and Upload to S3 <a class=\"anchor\" id=\"save_tts\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tts to S3\n",
    "local_file = \"tts.csv\"\n",
    "# Save merged file locally\n",
    "tts.to_csv(local_file, header=False, index=False)\n",
    "\n",
    "key = f\"{s3_prefix}/tts.csv\"\n",
    "boto3.Session().resource('s3').Bucket(s3_bucket_name).Object(key).upload_file(local_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Prepare Forecast Access to S3 <a class=\"anchor\" id=\"forecast_role\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iam = boto3.client(\"iam\")\n",
    "forecast_role_name = \"ForecastToS3\"\n",
    "\n",
    "create_role_response = iam.create_role(\n",
    "    RoleName=forecast_role_name,\n",
    "    AssumeRolePolicyDocument=json.dumps({\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Principal\": {\n",
    "                    \"Service\": \"forecast.amazonaws.com\",\n",
    "                },\n",
    "                \"Action\": \"sts:AssumeRole\",\n",
    "            },\n",
    "        ]\n",
    "    }),\n",
    ")\n",
    "\n",
    "forecast_role_arn = create_role_response[\"Role\"][\"Arn\"]\n",
    "print(forecast_role_arn)\n",
    "\n",
    "# Note that AmazonForecastFullAccess provides access to some specifically-named default S3 buckets as well,\n",
    "# but we just want it for the Forecast permissions themselves:\n",
    "iam.attach_role_policy(\n",
    "    RoleName=forecast_role_name,\n",
    "    PolicyArn=\"arn:aws:iam::aws:policy/AmazonForecastFullAccess\",\n",
    ")\n",
    "\n",
    "# By default (since we're experimenting), this code attaches over-generous S3 permissions (full access):\n",
    "iam.attach_role_policy(\n",
    "    RoleName=forecast_role_name,\n",
    "    PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3FullAccess\",\n",
    ")\n",
    "# You could instead use something like the below to give access to *only* the relevant buckets:\n",
    "# inline_s3_policy = {\n",
    "#     \"Version\": \"2012-10-17\",\n",
    "#     \"Statement\": [\n",
    "#         {\n",
    "#             \"Effect\": \"Allow\",\n",
    "#             \"Action\": \"s3:*\",\n",
    "#             \"Resource\": [\n",
    "#                 # (Assuming you're not running in a different partition e.g. aws-cn)\n",
    "#                 f\"arn:aws:s3:::{bucket_name}\",\n",
    "#                 f\"arn:aws:s3:::{bucket_name}/*\",\n",
    "#             ]\n",
    "#         },\n",
    "#     ],\n",
    "# }\n",
    "# if bucket_name != export_bucket_name:\n",
    "#     inline_s3_policy[\"Statement\"][0][\"Resource\"].append(f\"arn:aws:s3:::{export_bucket_name}\")\n",
    "#     inline_s3_policy[\"Statement\"][0][\"Resource\"].append(f\"arn:aws:s3:::{export_bucket_name}/*\")\n",
    "\n",
    "# iam.put_role_policy(\n",
    "#     RoleName=role_name,\n",
    "#     PolicyName=\"ForecastPoCBucketAccess\",\n",
    "#     PolicyDocument=json.dumps(inline_s3_policy)\n",
    "# )\n",
    "\n",
    "# IAM policy attachments *may* take up to a minute to propagate, so just to be safe:\n",
    "sleep(60) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save variables for use with other notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store df_raw\n",
    "%store s3_bucket_name\n",
    "%store s3_prefix\n",
    "%store start_time\n",
    "%store end_time\n",
    "%store item_id\n",
    "%store target_value\n",
    "%store timestamp\n",
    "%store FORECAST_FREQ\n",
    "%store forecast_dims"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:ap-southeast-1:492261229750:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
