{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data used in these notebooks: NYC Taxi trips open data\n",
    "\n",
    "Given hourly historical taxi trips data for NYC, your task is to predict #pickups in next 7 days, per hour and per pickup zone.  <br>\n",
    "\n",
    "<ul>\n",
    "<li>Original data source:  <a href=\"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\" target=\"_blank\"> https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page</a> </li>\n",
    "<li>AWS-hosted public source:  <a href=\"https://registry.opendata.aws/nyc-tlc-trip-records-pds/\" target=\"_blank\">https://registry.opendata.aws/nyc-tlc-trip-records-pds/ </a> </li>\n",
    "<li>AWS managed weather data ingestion as a service that is bundled with Amazon Forecast, aggregated by location and by hour.  Initially only for USA and Europe, but depending on demand, possibly in the future for other global regions. </li>\n",
    "<li>Data used:  Yellow taxis dates: 2018-12 through 2020-02 to avoid COVID effects </li>\n",
    "</ul>\n",
    "\n",
    " \n",
    "### Features and cleaning\n",
    "Note: ~5GB Raw Data has already been cleaned and joined using AWS Glue (tutorials to be created in future). \n",
    "<ul>\n",
    "    <li>Join shape files Latitude, Longitude</li>\n",
    "    <li>Add Trip duration in minutes</li>\n",
    "    <li>Drop negative trip distances, 0 fares, 0 passengers, less than 1min trip durations </li>\n",
    "    <li>Drop 2 unknown zones ['264', '265']\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0:  Set up and install libraries <a class=\"anchor\" id=\"setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard open libraries\n",
    "import pandas as pd\n",
    "\n",
    "# AWS libraries and initialization\n",
    "import boto3\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Define S3 bucket</b></br>\n",
    "The cell below will use the default SageMaker S3 bucket in the account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using folder 'nyc-taxi-trips'\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import get_execution_role\n",
    "sess = sagemaker.Session()\n",
    "s3_bucket_name = sess.default_bucket()\n",
    "    \n",
    "# create prefix for organizing your new bucket\n",
    "s3_prefix = \"nyc-taxi-trips\"\n",
    "print(f\"using folder '{s3_prefix}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Read data <a class=\"anchor\" id=\"read\"></a>\n",
    "\n",
    "The first thing we're going to do is read the headerless .csv file.  Then we need to identify which columns map to required Amazon Forecast inputs.\n",
    "\n",
    "<img src=\"https://amazon-forecast-samples.s3-us-west-2.amazonaws.com/common/images/nyctaxi_map_fields.png\" width=\"82%\">\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1507488, 9)\n",
      "(1507488, 9)\n",
      "Min timestamp = 2019-07-01 00:00:00\n",
      "Max timestamp = 2020-02-29 23:00:00\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pulocationid</th>\n",
       "      <th>pickup_hourly</th>\n",
       "      <th>pickup_day_of_week</th>\n",
       "      <th>day_hour</th>\n",
       "      <th>trip_quantity</th>\n",
       "      <th>mean_item_loc_weekday</th>\n",
       "      <th>pickup_geolocation</th>\n",
       "      <th>pickup_borough</th>\n",
       "      <th>binned_max_item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>140229</th>\n",
       "      <td>92</td>\n",
       "      <td>2019-09-17 20:00:00</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>Tuesday_20</td>\n",
       "      <td>0</td>\n",
       "      <td>1.21637</td>\n",
       "      <td>40.76412728_-73.83044713</td>\n",
       "      <td>Queens</td>\n",
       "      <td>Cat_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1180043</th>\n",
       "      <td>179</td>\n",
       "      <td>2019-10-15 01:00:00</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>Tuesday_1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.14946</td>\n",
       "      <td>40.77142534_-73.92681236</td>\n",
       "      <td>Queens</td>\n",
       "      <td>Cat_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1039834</th>\n",
       "      <td>118</td>\n",
       "      <td>2019-08-30 10:00:00</td>\n",
       "      <td>Friday</td>\n",
       "      <td>Friday_10</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>40.58563106_-74.13707013</td>\n",
       "      <td>Staten Island</td>\n",
       "      <td>Cat_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528615</th>\n",
       "      <td>152</td>\n",
       "      <td>2019-10-15 21:00:00</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>Tuesday_21</td>\n",
       "      <td>7</td>\n",
       "      <td>7.35476</td>\n",
       "      <td>40.8175772_-73.95432487</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Cat_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027600</th>\n",
       "      <td>208</td>\n",
       "      <td>2019-10-13 19:00:00</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>Sunday_19</td>\n",
       "      <td>0</td>\n",
       "      <td>1.08571</td>\n",
       "      <td>40.82468614_-73.82488634</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>Cat_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        pulocationid       pickup_hourly pickup_day_of_week    day_hour  trip_quantity  mean_item_loc_weekday        pickup_geolocation pickup_borough binned_max_item\n",
       "140229            92 2019-09-17 20:00:00            Tuesday  Tuesday_20              0                1.21637  40.76412728_-73.83044713         Queens           Cat_1\n",
       "1180043          179 2019-10-15 01:00:00            Tuesday   Tuesday_1              1                4.14946  40.77142534_-73.92681236         Queens           Cat_1\n",
       "1039834          118 2019-08-30 10:00:00             Friday   Friday_10              0                1.00000  40.58563106_-74.13707013  Staten Island           Cat_1\n",
       "528615           152 2019-10-15 21:00:00            Tuesday  Tuesday_21              7                7.35476   40.8175772_-73.95432487      Manhattan           Cat_1\n",
       "1027600          208 2019-10-13 19:00:00             Sunday   Sunday_19              0                1.08571  40.82468614_-73.82488634          Bronx           Cat_1"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Read cleaned, joined, featurized data from Glue ETL processing\n",
    "df_raw = pd.read_csv(\"s3://amazon-forecast-samples/data_prep_templates/clean_features.csv\"\n",
    "                          , parse_dates=True\n",
    "                          , header=None\n",
    "                          , dtype={0:'str'\n",
    "                                   , 1: 'str'\n",
    "                                   , 2: 'str'\n",
    "                                   , 3:'str'\n",
    "                                   , 4: 'int32'\n",
    "                                   , 5: 'float64'\n",
    "                                   , 6: 'str'\n",
    "                                   , 7: 'str'\n",
    "                                   , 8: 'str'}\n",
    "                          , names=['pulocationid', 'pickup_hourly', 'pickup_day_of_week'\n",
    "                                   , 'day_hour', 'trip_quantity', 'mean_item_loc_weekday'\n",
    "                                   , 'pickup_geolocation', 'pickup_borough', 'binned_max_item'])\n",
    "\n",
    "# drop duplicates\n",
    "print(df_raw.shape)\n",
    "df_raw.drop_duplicates(inplace=True)\n",
    "\n",
    "df_raw['pickup_hourly'] = pd.to_datetime(df_raw[\"pickup_hourly\"], format=\"%Y-%m-%d %H:%M:%S\", errors='coerce')\n",
    "print(df_raw.shape)\n",
    "start_time = df_raw.pickup_hourly.min()\n",
    "end_time = df_raw.pickup_hourly.max()\n",
    "print(f\"Min timestamp = {start_time}\")\n",
    "print(f\"Max timestamp = {end_time}\")\n",
    "df_raw.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Transform Data <a class=\"anchor\" id=\"transform_tts\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forecast_dims = ['pickup_hourly', 'pulocationid']\n"
     ]
    }
   ],
   "source": [
    "# map expected column names\n",
    "item_id = \"pulocationid\"\n",
    "target_value = \"trip_quantity\"\n",
    "timestamp = \"pickup_hourly\"\n",
    "\n",
    "# forecast setting\n",
    "FORECAST_FREQ = \"H\"\n",
    "\n",
    "# specify array of dimensions you'll use for forecasting\n",
    "forecast_dims = [timestamp, item_id]\n",
    "\n",
    "print(f\"forecast_dims = {forecast_dims}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start date = 2019-07-01 00:00:00\n",
      "end date = 2020-02-29 23:00:00\n",
      "(1507488, 3)\n",
      "pickup_hourly    datetime64[ns]\n",
      "pulocationid             object\n",
      "trip_quantity             int32\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_hourly</th>\n",
       "      <th>pulocationid</th>\n",
       "      <th>trip_quantity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-07-02 09:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-07-03 01:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-07-05 06:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-07-06 08:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-07-26 17:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        pickup_hourly pulocationid  trip_quantity\n",
       "0 2019-07-02 09:00:00            1              0\n",
       "1 2019-07-03 01:00:00            1              0\n",
       "2 2019-07-05 06:00:00            1              0\n",
       "3 2019-07-06 08:00:00            1              0\n",
       "4 2019-07-26 17:00:00            1              0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Assemble TTS required columns\n",
    "\n",
    "tts = df_raw[[timestamp, item_id, target_value]].copy()\n",
    "\n",
    "print(f\"start date = {tts[timestamp].min()}\")\n",
    "print(f\"end date = {tts[timestamp].max()}\")\n",
    "\n",
    "# check it\n",
    "print(tts.shape)\n",
    "print(tts.dtypes)\n",
    "tts.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 3. Save Data and Upload to S3 <a class=\"anchor\" id=\"save_tts\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tts to S3\n",
    "local_file = \"tts.csv\"\n",
    "# Save merged file locally\n",
    "tts.to_csv(local_file, header=False, index=False)\n",
    "\n",
    "key = f\"{s3_prefix}/tts.csv\"\n",
    "boto3.Session().resource('s3').Bucket(s3_bucket_name).Object(key).upload_file(local_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save variables for use with other notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'df_raw' (DataFrame)\n",
      "Stored 's3_bucket_name' (str)\n",
      "Stored 's3_prefix' (str)\n",
      "Stored 'start_time' (Timestamp)\n",
      "Stored 'end_time' (Timestamp)\n",
      "Stored 'item_id' (str)\n",
      "Stored 'target_value' (str)\n",
      "Stored 'timestamp' (str)\n",
      "Stored 'FORECAST_FREQ' (str)\n",
      "Stored 'forecast_dims' (list)\n"
     ]
    }
   ],
   "source": [
    "%store df_raw\n",
    "%store s3_bucket_name\n",
    "%store s3_prefix\n",
    "%store start_time\n",
    "%store end_time\n",
    "%store item_id\n",
    "%store target_value\n",
    "%store timestamp\n",
    "%store FORECAST_FREQ\n",
    "%store forecast_dims"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:ap-southeast-1:492261229750:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
